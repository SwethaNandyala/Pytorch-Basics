{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59327914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c0190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0,requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173841a",
   "metadata": {},
   "source": [
    "# Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba26fa",
   "metadata": {},
   "source": [
    "## compute output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88d1f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = w*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa194e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y pred tensor(1., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Y pred\",y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86067a",
   "metadata": {},
   "source": [
    "## compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5dd970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (y_hat - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f34ec6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534600d",
   "metadata": {},
   "source": [
    "# Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d071670",
   "metadata": {},
   "source": [
    "## compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "188ba048",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2de48ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial gradient tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "print(\"initial gradient\",w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193a232",
   "metadata": {},
   "source": [
    "### manual way of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a3630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before running epochs 0.0\n",
      "epoch 1 W 1.200 Loss 120.000\n",
      "epoch 2 W 1.680 Loss 19.200\n",
      "epoch 3 W 1.872 Loss 3.072\n",
      "epoch 4 W 1.949 Loss 0.492\n",
      "epoch 5 W 1.980 Loss 0.079\n",
      "epoch 6 W 1.992 Loss 0.013\n",
      "epoch 7 W 1.997 Loss 0.002\n",
      "epoch 8 W 1.999 Loss 0.000\n",
      "epoch 9 W 1.999 Loss 0.000\n",
      "epoch 10 W 2.000 Loss 0.000\n",
      "Prediction after running 10 epochs =  9.998951268196105\n"
     ]
    }
   ],
   "source": [
    "#lets assume y = 2*x is the function so model has to learn that the wt is 2 from its trainings\n",
    "\n",
    "#inputs and outputs\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0],dtype = 'float32')\n",
    "y = np.array([2.0, 4.0, 6.0, 8.0],dtype = 'float32')\n",
    "\n",
    "#weights\n",
    "w = 0.0\n",
    "\n",
    "#learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "#predictions in forward pass\n",
    "def forward(x):\n",
    "    \n",
    "    y_pred = w*x\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "#Compute loss\n",
    "def loss(y_pred,y):\n",
    "    \n",
    "    loss = ((y_pred - y)**2).sum()  \n",
    "    \n",
    "    return loss\n",
    "\n",
    "#Loss = (w.X - Y)**2\n",
    "#Loss = (Y_pred - Y)**2\n",
    "\n",
    "#Compute gradients\n",
    "#DL/DW = 1/N * 2.X * (w.X - Y)\n",
    "\n",
    "def gradient(x,y,y_pred):\n",
    "    \n",
    "    grad = np.dot((2*x),(y_pred - y)).mean()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "no_of_epochs = 10\n",
    "print(\"Prediction before running epochs\",forward(5))\n",
    "\n",
    "for i in range(no_of_epochs):\n",
    "    \n",
    "    #forwardpass\n",
    "    y_pred = forward(x)\n",
    "    \n",
    "    #compute loss\n",
    "    \n",
    "    loss2 = loss(y_pred,y)\n",
    "    \n",
    "    #compute gradients\n",
    "    dw = gradient(x,y,y_pred)\n",
    "        \n",
    "    w-= learning_rate*dw\n",
    "    print(f\"epoch {i+1} W {w:.3f} Loss {loss2:.3f}\" ) \n",
    "    \n",
    "print(f\"Prediction after running {no_of_epochs} epochs = \",forward(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c8c869",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86834ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before running epochs tensor(0., grad_fn=<MulBackward0>)\n",
      "epoch 1 W 1.200 Loss 120.000\n",
      "epoch 2 W 1.680 Loss 19.200\n",
      "epoch 3 W 1.872 Loss 3.072\n",
      "epoch 4 W 1.949 Loss 0.492\n",
      "epoch 5 W 1.980 Loss 0.079\n",
      "epoch 6 W 1.992 Loss 0.013\n",
      "epoch 7 W 1.997 Loss 0.002\n",
      "epoch 8 W 1.999 Loss 0.000\n",
      "epoch 9 W 1.999 Loss 0.000\n",
      "epoch 10 W 2.000 Loss 0.000\n",
      "Prediction after running 10 epochs =  tensor(9.9990, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#lets assume y = 2*x is the function so model has to learn that the wt is 2 from its trainings\n",
    "\n",
    "#inputs and outputs\n",
    "x = torch.tensor([[1.0],[ 2.0], [3.0], [4.0]],dtype = torch.float32)\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]],dtype = torch.float32)\n",
    "\n",
    "#weights\n",
    "w = torch.tensor(0.0,dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "#predictions in forward pass\n",
    "def forward(x):\n",
    "    \n",
    "    y_pred = w*x\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "no_of_epochs = 10\n",
    "print(\"Prediction before running epochs\",forward(5))\n",
    "\n",
    "for i in range(no_of_epochs):\n",
    "    \n",
    "    #forwardpass\n",
    "    y_pred = forward(x)\n",
    "    \n",
    "    #compute loss\n",
    "    \n",
    "    loss2 = loss(y_pred,y)\n",
    "    \n",
    "    #compute gradients\n",
    "    loss2.backward()  #calculates the gradient of loss wrt w dl/dW for backpropogation\n",
    "        \n",
    "    #this should not be part of computational graph\n",
    "    with torch.no_grad():\n",
    "        w-= learning_rate*w.grad\n",
    "    \n",
    "    #zero out the gradient\n",
    "    #else whenever we call bcakward it will accumulte the gradient in w.grad attribute\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    print(f\"epoch {i+1} W {w:.3f} Loss {loss2:.3f}\" ) \n",
    "    \n",
    "print(f\"Prediction after running {no_of_epochs} epochs = \",forward(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f05ea",
   "metadata": {},
   "source": [
    "## Pytorch loss and Pytorch Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da69eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before running epochs 0.0\n",
      "epoch 1 W 0.300 Loss 30.000\n",
      "epoch 11 W 1.665 Loss 1.163\n",
      "epoch 21 W 1.934 Loss 0.045\n",
      "epoch 31 W 1.987 Loss 0.002\n",
      "epoch 41 W 1.997 Loss 0.000\n",
      "epoch 51 W 1.999 Loss 0.000\n",
      "epoch 61 W 2.000 Loss 0.000\n",
      "epoch 71 W 2.000 Loss 0.000\n",
      "epoch 81 W 2.000 Loss 0.000\n",
      "epoch 91 W 2.000 Loss 0.000\n",
      "Prediction after running 100 epochs =  9.999998092651367\n"
     ]
    }
   ],
   "source": [
    "#lets assume y = 2*x is the function so model has to learn that the wt is 2 from its trainings\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam,SGD\n",
    "#inputs and outputs\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0]],dtype = torch.float32)\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]],dtype = torch.float32)\n",
    "\n",
    "#weights\n",
    "w = torch.tensor(0.0,dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "#Pytorch loss and optimiser\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "opt = SGD(params=[w], lr = learning_rate)\n",
    "\n",
    "#predictions in forward pass\n",
    "def forward(x):\n",
    "    \n",
    "    y_pred = w*x\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "no_of_epochs = 100\n",
    "print(\"Prediction before running epochs\",forward(5).item())\n",
    "\n",
    "for i in range(no_of_epochs):\n",
    "    \n",
    "    #forwardpass\n",
    "    y_pred = forward(x)\n",
    "    \n",
    "    #compute loss\n",
    "    \n",
    "    l = loss(y_pred,y)\n",
    "    \n",
    "    #compute gradients\n",
    "    l.backward()  #calculates the gradient of loss wrt w dl/dW for backpropogation\n",
    "        \n",
    "    #update the model parameters or weights\n",
    "    \n",
    "    opt.step()\n",
    "    \n",
    "    #zero out the gradient\n",
    "    #else whenever we call bcakward it will accumulte the gradient in w.grad attribute\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    if (i%10 == 0):\n",
    "        print(f\"epoch {i+1} W {w:.3f} Loss {l:.3f}\" ) \n",
    "    \n",
    "print(f\"Prediction after running {no_of_epochs} epochs = \",forward(5).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdbf8be",
   "metadata": {},
   "source": [
    "## forward pass using pytorc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71bd26e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 1.696\n",
      "epoch  1 : w =  0.4925873875617981  loss =  tensor(19.3001, grad_fn=<MseLossBackward0>)\n",
      "epoch  11 : w =  1.5042155981063843  loss =  tensor(0.6296, grad_fn=<MseLossBackward0>)\n",
      "epoch  21 : w =  1.6744176149368286  loss =  tensor(0.1389, grad_fn=<MseLossBackward0>)\n",
      "epoch  31 : w =  1.70905601978302  loss =  tensor(0.1191, grad_fn=<MseLossBackward0>)\n",
      "epoch  41 : w =  1.721674919128418  loss =  tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
      "epoch  51 : w =  1.7305437326431274  loss =  tensor(0.1054, grad_fn=<MseLossBackward0>)\n",
      "epoch  61 : w =  1.738607406616211  loss =  tensor(0.0992, grad_fn=<MseLossBackward0>)\n",
      "epoch  71 : w =  1.7463452816009521  loss =  tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
      "epoch  81 : w =  1.753840684890747  loss =  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
      "epoch  91 : w =  1.7611124515533447  loss =  tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
      "Prediction before training: f(5) = 9.521\n"
     ]
    }
   ],
   "source": [
    "#lets assume y = 2*x is the function so model has to learn that the wt is 2 from its trainings\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam,SGD\n",
    "#inputs and outputs\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0]],dtype = torch.float32)\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]],dtype = torch.float32)\n",
    "\n",
    "x_test = torch.tensor([5.0],dtype = torch.float32)\n",
    "\n",
    "n_samples , n_features = x.shape\n",
    "#learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "#Pytorch loss and optimiser\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "#predictions in forward pass\n",
    "model = nn.Linear(in_features = n_features ,out_features =n_features)\n",
    "opt = SGD(params=model.parameters(), lr = learning_rate)\n",
    "\n",
    "no_of_epochs = 100\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "for i in range(no_of_epochs):\n",
    "    \n",
    "    #forwardpass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    #compute loss\n",
    "    \n",
    "    l = loss(y_pred,y)\n",
    "    \n",
    "    #compute gradients\n",
    "    l.backward()  #calculates the gradient of loss wrt w dl/dW for backpropogation\n",
    "        \n",
    "    #update the model parameters or weights\n",
    "    \n",
    "    opt.step()\n",
    "    \n",
    "    #zero out the gradient\n",
    "    #else whenever we call bcakward it will accumulte the gradient in w.grad attribute\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    if (i%10 == 0):\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print('epoch ', i+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "        \n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70d95271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 4.663\n",
      "epoch  1 : w =  1.0905404090881348  loss =  tensor(8.5253, grad_fn=<MseLossBackward0>)\n",
      "epoch  11 : w =  1.7618355751037598  loss =  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
      "epoch  21 : w =  1.87252676486969  loss =  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "epoch  31 : w =  1.8929650783538818  loss =  tensor(0.0158, grad_fn=<MseLossBackward0>)\n",
      "epoch  41 : w =  1.8988085985183716  loss =  tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "epoch  51 : w =  1.9022289514541626  loss =  tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "epoch  61 : w =  1.9051865339279175  loss =  tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
      "epoch  71 : w =  1.9079984426498413  loss =  tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "epoch  81 : w =  1.9107178449630737  loss =  tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "epoch  91 : w =  1.9133553504943848  loss =  tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "Prediction before training: f(5) = 9.826\n"
     ]
    }
   ],
   "source": [
    "#lets assume y = 2*x is the function so model has to learn that the wt is 2 from its trainings\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam,SGD\n",
    "#inputs and outputs\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0]],dtype = torch.float32)\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]],dtype = torch.float32)\n",
    "\n",
    "x_test = torch.tensor([5.0],dtype = torch.float32)\n",
    "\n",
    "n_samples , n_features = x.shape\n",
    "#learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "#Pytorch loss and optimiser\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "#predictions in forward pass\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        \n",
    "        self.in_features = inputs\n",
    "        self.out_features = outputs\n",
    "        self.ln = nn.Linear(self.in_features ,self.out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = LinearRegression(n_features,n_features)\n",
    "opt = SGD(params=model.parameters(), lr = learning_rate)\n",
    "\n",
    "no_of_epochs = 100\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "for i in range(no_of_epochs):\n",
    "    \n",
    "    #forwardpass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    #compute loss\n",
    "    \n",
    "    l = loss(y_pred,y)\n",
    "    \n",
    "    #compute gradients\n",
    "    l.backward()  #calculates the gradient of loss wrt w dl/dW for backpropogation\n",
    "        \n",
    "    #update the model parameters or weights\n",
    "    \n",
    "    opt.step()\n",
    "    \n",
    "    #zero out the gradient\n",
    "    #else whenever we call bcakward it will accumulte the gradient in w.grad attribute\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    if (i%10 == 0):\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print('epoch ', i+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "        \n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
